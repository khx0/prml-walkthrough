%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% author: Nikolas Schnellbaecher
% contact: khx0@posteo.net
% file: main.tex
% date: 2020-05-08
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt, DINA4, fleqn]{amsart}

\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage{lmodern}

% math packages
\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage{bm}

\usepackage{wasysym}
\usepackage{blindtext}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{fancybox}
\usepackage{framed}
%\usepackage{colortbl}

\usepackage{tabularx}
\usepackage{multirow}

\usepackage{listings} 
\lstset{numbers = left, numberstyle = \tiny, numbersep = 5pt} 
\lstset{language = c++} 

\usepackage{geometry}
\geometry{hmargin = 2.5cm,
		  vmargin = {2cm, 3.0cm},
	  	  footskip = 1.0cm}




% color packages
\usepackage{color}
\usepackage{graphicx}
\usepackage[table]{xcolor}
%\usepackage[table]{xcolor}
\usepackage{tcolorbox}

\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}
\definecolor{darkgreen}{rgb}{0.14, 0.72, 0.31}
\definecolor{MyBoxColor}{rgb}{0.9,0.9,0.9}
% \definecolor{CiteBlue}{RGB}{28, 58, 189}
\definecolor{CiteBlue}{RGB}{31, 119, 180} % mpl C0 color

\newenvironment{shadedSmaller}{
  \def\FrameCommand{\fboxsep=\FrameSep \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-2\width\FrameRestore}}
{\endMakeFramed}

\newtcolorbox{mybox_tc2}[1]{colback=red!5!white,colframe=red!75!black,fonttitle=\bfseries,title=#1}

\newtcolorbox{mybox_tc3}[1]{colback=darkgreen!5!white,colframe=darkgreen!75!black,fonttitle=\bfseries,title=#1}

\newenvironment{shadedSmallerPadding}{
  \def\FrameCommand{\fboxsep=0.15cm \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-1.1\width\FrameRestore}}
{\endMakeFramed}

\usepackage{fancyhdr}

\setlength{\headheight}{15pt}


\pagestyle{fancy}
\fancyhf{}
\fancyhead{}
\fancyfoot[RO]{\sffamily\bfseries\vrule width 0.8pt\hskip1mm\thepage}
\fancyfoot[LE]{\sffamily\bfseries\thepage \hskip1mm \vrule width 0.8pt}

\fancypagestyle{plain}{%
	\renewcommand{\headrulewidth}{0pt}%
	\fancyhf{}%
	\fancyfoot[LE,RO]{\sffamily\bfseries\vrule width 0.8pt\hskip1mm\thepage}%
}

\renewcommand{\headrulewidth}{0pt}

\usepackage{faktor}


% load hyperref package as last package
\usepackage{hyperref} 

\hypersetup{
	colorlinks,
	linkcolor= CiteBlue,
	citecolor = CiteBlue,
	urlcolor = CiteBlue
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% custom definitions
\def\vw{\boldsymbol{w}\xspace}
\def\vb{\boldsymbol{b}\xspace}
\def\vy{\boldsymbol{y}\xspace}
\def\mA{\boldsymbol{A}\xspace}
\def\mV{\boldsymbol{V}\xspace}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagestyle{fancy}

\begin{flushleft}
{\sc \Large Supplemental Notes on Decision Theory} \hfill \today \\
\medskip
Nikolas Schnellb√§cher \underline{\hspace{6.53in}} \\
\end{flushleft}

\section{ad figure 1.24 till figure 1.27}
These notes cover some clarifying remarks about the part on decision theory in chapter 1 and the accompanying figures 1.24, 1.26 and 1.27. In the covered explanatory example the problem considered consists of two random variables,
a continuous input variable $X$ on a domain $x\in\mathcal{X}$ and a discrete random variable $\mathcal{C}$ with two states $C_k\in \{C_1, C_2 \}$. The decision problem is to classify if a given input $x$ is either of the two binary states.

\subsection{Supplemental remarks on figure 1.24}
The plot in figure 1.24 displays two curves for $p(x \, | \, C_1)$ and $p(x \, | \, C_2)$. First of all make sure, that you appreciate, that these two curves combined
form the full joint probability distribution $p(x, \, C)$ of this problem. And as usual every quantity can be derived from having full information about the underlying joint distribution. Although the axes in figure 1.24 do not reveal any scale, it is useful to reiterate that each of these curves $p(x, \, C_k)$ for $k\in\{1, 2\}$ individually is not normalized but only the full joint is normalized, such that
\begin{align}
\mathcal{N} = \sum_{k=1}^{2} \, \, \int\displaylimits_{\mathcal{X}} \, p(x, C_k) \, dx = 1
\end{align}
is satisfied. The integral of the two shown curved individually is simply a standard marginalization and yields
\begin{align}
\int\displaylimits_{\mathcal{X}} p(x, C_1) \, dx = p(C_1) \quad  \text{and} \quad
\int\displaylimits_{\mathcal{X}} p(x, C_2) \, dx = p(C_2).
\end{align}
The knowledge about $p(C_1)$ and $p(C_2)$ is, in this simple example, of course the full marginalized distribution $p(C)$, which is itself of course also properly normalized, such that
\begin{align}
\sum_{k=1}^{2} p(C_k) = p(C_1) + p(C_2) = 1
\end{align}
holds.

The marginalization to obtain $p(x)$ from the joint is here also trivial and simply obtained by summing
\begin{align}
p(x) = \sum_{k=1}^{2} p(x, C_k) = p(x, C_1) + p(x, C_2) \, ,
\label{eq:marginal_x}
\end{align}
which is once again a properly normalized distribution of $x$ itself, such that
\begin{align}
\int\displaylimits_{\mathcal{X}} p(x) \, dx = 1.
\end{align}
Long story short: Having the joint we can immediately get the marginalized distributions and check that both the joint and (both) marginalized distributions are indeed all normalized.

\subsection{Supplemental remarks on figure 1.26}
In Figure 1.26 the chapter discusses the posterior conditional distributions $p(C_k \, | \, x)$ as a function of $x$.
Remember, this is the probability for the class $C_k$ given that the input variable takes the value $x$. Viewed as a function of $x$ we can actually straight forward write down how to obtain this conditional distribution, starting from the full joint distribution $p(x, C)$ as shown in figure 1.24.
For this consider having both $p(x, C_1)$ and $p(x, C_2)$ available. For a given input value $x$, the probability of belonging to class $C_1$ is then simply given by the value of $p(x, C_1)$ at this $x$ value, weighted by the total statistical weight of both classes $p(x, C_1) + p(x,C_2)$ at this value $x$,
\textit{i.e.} we arrive at 
\begin{align}
p(C_1 | x) = \dfrac{p(x, C_1)}{p(x, C_1) + p(x, C_2)} = \dfrac{p(x, C_1)}{p(x)} \, ,
\label{eq:figure_1.26_equation}
\end{align}
which we could rewrite using the marginal distribution $p(x)$ as stated in equation \eqref{eq:marginal_x}.
This relation (equation \eqref{eq:figure_1.26_equation}) can be used to compute the results for figure 1.26 given the data from figure 1.24.
In general we thus have
\begin{align}
p(C_k \, |\, x) = \dfrac{p(x, C_k)}{p(x)} \, \, , \quad \text{for} \quad k\in\{1,2\}\, .
\label{eq:figure_1.26_equation_mk2}
\end{align}
If you should find this argument too ad hoc, we can also derive at the identical result by combining \emph{Bayes' theorem} with the product rule of probability theory.
The product rule applied to the quantities in this problem read
\begin{align}
p(x, C_1) &= p(x\, | \, C_1) \cdot p(C_1) \quad \text{and} \\
p(x, C_2) &= p(x\, | \, C_2) \cdot p(C_2) \, .
\label{eq:product_rule}
\end{align}
Using \emph{Bayes'} theorem we can express the wanted posterior conditional distribution $p(C_k | x)$ in the following way
\begin{align}
p(C_k | x) = \dfrac{p(x | C_k) \cdot p(C_k)}{p(x)} = \dfrac{p(x, C_k)}{p(C_k)} \cdot \dfrac{p(C_k)}{p(x)} = \dfrac{p(x, C_k)}{p(x)} \, , \quad \text{for} \quad k\in\{1,2\} \, .
\end{align}
This result is identical to equation \eqref{eq:figure_1.26_equation_mk2} and states how to compute posterior conditional distributions from the joint.

\subsection{Supplemental remarks on figure 1.27}
Figure 1.27 in chapter 1 depicts a similar example as figure 1.24 and 1.26, but in contrast to figure 1.24 here we start from a given prior class conditional distribution $p(x \, | C_k)$ (see figure 1.27 left) and not from the full joint probability distribution $p(x, C)$ as we did in figure 1.24.

Before we explain how to derive the same conditional posterior distributions $p(C_k \, | \, x)$ in this case, we quickly reiterate, that both conditional distributions, namely the $p(x \, | \, C_k)$'s and the $p(C_k \, | \, x)$'s are all properly normalized probability distributions individually.
We start by writing down the marginalization of the joint distribution by integrating out the independent input variable $x$.
\begin{align}
p(C_k) &= \int\displaylimits_{\mathcal{X}} p(x, C_k) \, dx \\
&= \int\displaylimits_{\mathcal{X}} p(x \, | \, C_k) \cdot p(C_k) \, dx
= p(C_k) \int\displaylimits_{\mathcal{X}} p(x \, | \, C_k) \, dx \quad \text{for} \quad k\in\{1, 2\}
\end{align}
Next, we can cancel the $p(C_k)$ term on both the left and right hand sides and obtain
\begin{align}
1 = \int\displaylimits_{\mathcal{X}} p(x \, | \, C_k) \, dx \, \quad \text{for} \quad k\in\{1, 2\} .
\end{align}
This is the normalization condition for the conditional distributions $p(x \, | \, C_k)$ for all $k$.
The case of $p(C_k) = 0$ can be omitted, since this would anyway be a pathological example.
Analog, we derive the normalization condition for the second type of conditional distributions $p(C_k \, | \, x)$ that we encounter in this example. As before we start by marginalization of the joint, but this time marginalizing the class variable $C_k$.
\begin{align}
p(x) = \sum_{k = 1}^{2} p(x, C_k) = \sum_{k = 1}^{2} p(C_k \, | \, x) \cdot p(x) = p(x)\sum_{k = 1}^{2} p(C_k \, | \, x)
\end{align}
As before, we can cancel the $p(x)$ on both sides an obtain
\begin{align}
1 = \sum_{k = 1}^{2} p(C_k \, | \, x) \, ,
\end{align}
which is the normalization condition for the conditional distribution $p(C_k \, | \, x)$.

Now we can discuss how to compute the conditional posterior distributions $p(C_k \, | \, x)$ from the given conditional class prior distributions $p(x \, | \, C_k)$, as they are for example shown in the left half of figure 1.27.
First of all notice, that the joint distribution $p(x, C_k)$ (as given in figure 1.24) and the conditional class priors $p(x \, | \, C_k)$ are related to each other, by the product rule as previously stated in equation \eqref{eq:product_rule}, \textit{i.e.} they differ by the marginal distribution for the classes $p(C_k)$.







\newpage


\begin{mybox_tc3}{Probabilistic curve fitting - maximum likelihood approach}
We consider dataset $\{\boldsymbol{\mathsf{x}}, \boldsymbol{\mathsf{t}}\}$
and a (polynomial) model function $y(x,\bm{w})$ and
assume that
\begin{itemize}
	\item the data points $\{(x_n, t_n)\}$ are statistically independent
	\item and that the target values $t_n$ are distributed according to
	\begin{align}
	t_n  \sim p(t_n \, | \, x_n, \bm{w}, \beta) = \mathcal{N}\left(
	t_n \, \bigl| \, y(x_n,\bm{w}), \beta^{-1}\right) 
	\end{align}
	a Gaussian distribution with mean $\mu = y(x_n, \bm{w})$ and variance $\sigma^2 = \beta^{-1}$.
\end{itemize}
	Then maximizing the likelihood (log likelihood), is equivalent to minimizing the \emph{sum-of-squares} error function
	\begin{align}
	E(\bm{w}) =\dfrac{1}{2} \sum\displaylimits_{n=1}^{N} \biggl(y(x_n, \bm{w}) - t_n\biggl)^2.
	\end{align}
The solution for the model parameters $\bm{w}_{\text{ML}}$ and $\beta_{\text{ML}}$ are then for a polynomial model function $y(x,\bm{w})$ given by
solving the linear system
\begin{align}
\mA \cdot \vw = \vb
\label{eq:linearSystem}
\end{align}
where
\begin{align}
A_{ij} = \sum \displaylimits_{n = 1}^{N} (x_n)^{i + j} \, , \quad
(\vw)_j = w_j \quad \text{and} \quad
b_i = \sum \displaylimits_{n = 1}^{N} (x_n)^{i} \, t_n \, ,
\end{align}
where both indices $i$ and $j$ run from $0, 1, 2, \dotsc, M$.
With the solution $\bm{w}_{\text{ML}}$, the maximum likelihood solution for the precision parameter $\beta_{\text{ML}}$ reads
\begin{align}
\dfrac{1}{\beta_{\text{ML}}} = \sigma^2_{\text{ML}}= \dfrac{1}{N} \sum\displaylimits_{n=1}^{N} \biggl(y(x_n, \bm{w}_{\text{ML}}) - t_n\biggl)^2
\end{align}
\end{mybox_tc3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.42\textwidth]{figure_1_16.pdf}
%	\caption{Schematic illustration of the uncertainty of the target variable $t$ in form of a Gaussian conditional distribution as specified in equation \eqref{eq:tUncertainty}, for which the mean is given by the polynomial function $y(x,\bm{w})$ and the precision is set by the parameter $\beta = 1/\sigma^2$.
%	\label{fig:figure_1_16}}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\rowcolors{1}{gray!25}{white}

\renewcommand{\arraystretch}{1.40}

\begin{center}
	\begin{table}
	\begin{tabularx}{8.0cm}{ccX}
		\hline
		query point $x$ & mean $m(x)$ & variance $s^2(x)$\\
		\hline
		$0$ & $1/5$ & $7/5$ \\  
		$0.5$ & $2/5$ & $27/20$ \\
		$1.0$ & $3/5$ & $8/5$ \\ 
		\hline  
	\end{tabularx}
	
	 \caption{Summary table.\label{tbl:benchmarkTable}}

	\end{table}
\end{center}

\end{document}
