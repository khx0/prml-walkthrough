\documentclass[11pt,DINA4, fleqn]{amsart}
\pagestyle{empty}
\newcommand{\R}{\Bbb{R}}
\newtheorem{thm}{Frage}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{bm} % bold math symbols

\usepackage{wasysym}
\usepackage[english]{babel}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{geometry}
\geometry{hmargin=2.5cm,vmargin={2cm,1cm}}

\usepackage{blindtext}
\usepackage{multirow,booktabs}

\usepackage{pifont}
\usepackage{dsfont}
\usepackage{enumitem}
%\setitemize{leftmargin=0.5cm} 
%\setlength{\itemindent}{1in}
\usepackage{fancybox,framed}

\usepackage{listings} \lstset{numbers=left, numberstyle=\tiny, numbersep=5pt} \lstset{language=c++} 

\usepackage{tcolorbox}

\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}

\usepackage{graphicx}

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.14,0.72,0.31}
\definecolor{MyBoxColor}{rgb}{0.9,0.9,0.9}
\newenvironment{shadedSmaller}{
  \def\FrameCommand{\fboxsep=\FrameSep \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-2\width\FrameRestore}}
{\endMakeFramed}

\newtcolorbox{mybox_tc2}[1]{colback=red!5!white,colframe=red!75!black,fonttitle=\bfseries,title=#1}

\newtcolorbox{mybox_tc3}[1]{colback=darkgreen!5!white,colframe=darkgreen!75!black,fonttitle=\bfseries,title=#1}


\newenvironment{shadedSmallerPadding}{
  \def\FrameCommand{\fboxsep=0.15cm \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-1.1\width\FrameRestore}}
{\endMakeFramed}

\usepackage{faktor}

\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

%----------------------------------------------------------------------------------------------------------------

\def\vw{\boldsymbol{w}\xspace}
\def\vb{\boldsymbol{b}\xspace}
\def\vy{\boldsymbol{y}\xspace}
\def\mA{\boldsymbol{A}\xspace}
\def\mV{\boldsymbol{V}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Ldot}{\overset{\textbf{.}\kern0.23em}{\mathbf{L}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%\begin{tikzpicture}[normal line/.style={->},font=\scriptsize] \path[normal line] (m-1-1) edge (m-1-2); %\end{tikzpicture}

\tikzset{node distance = 2cm, auto}


%                                      $\mathds{Z}/n\mathds{Z}$

\begin{flushleft}
{\sc \Large PRML Notes} \hfill \today \\
\medskip
Nikolas Schnellb√§cher \underline{\hspace{6.53in}} \\
\end{flushleft}

\section*{ad 1.2.5 Curve fitting revisited}
We consider a set of training data $\{\boldsymbol{\mathsf{x}}, \boldsymbol{\mathsf{t}}\}$, where
\begin{align}
\boldsymbol{\mathsf{x}} &= (x_1, \dots, x_N)^T \, , \\
\boldsymbol{\mathsf{t}} &= (t_1, \dots, t_N)^T
\end{align}
are the input variables $x_n$ and their corresponding target values $t_n$, respectively.
The central idea is using probability distribution to  express our uncertainty over the value of the target variable $t$, \textit{i.e.} we state
\begin{align}
p(t \, | \, x, \bm{w}, \beta) = \mathcal{N}\left(
t \, \bigl| \, y(x,\bm{w}), \beta^{-1}\right) \, .
\end{align}
The parameter $\beta$ is called \emph{precision} and correspond to the inverse variance of the distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{figure_1_16.pdf}
	\caption{Graphical intuition for the origin of bias in the maximum likelihood estimation of the variance. Consider three cases (A, B, C) where each sample consist of two data points (blue dots), drawn from (the same) true (unknown) underlying Gaussian distribution (green curve). For each case the red curve shows the inferred (fitted) Gaussian distribution based on the maximum likelihood estimators of the mean and the variance. Averaged across the three data sets, the mean will be estimated as the correct mean of the true Gaussian distribution, while the variance (averaged) across the three datasets will still significantly underestimate the true variance. This is due to the fact, that the variance for each subsample is measured as the sample variance with respect to the sample mean and not relative to the true mean. This figure corresponds to figure 1.15 from PRML by Bishop. In many copies I found, there was only a single blue dot (instead of two blue dots) per sample.
		\label{fig:figure_1_16}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In curve fitting, we use the existing training data $\{\boldsymbol{\mathsf{x}}, \boldsymbol{\mathsf{t}}\}$ to infer the unknown parameters $\bm{w}$ and $\beta$.
We will below show how this works using a \emph{Maximum Likelihood} ansatz.


\begin{mybox_tc3}{Curve fitting revisited}
The \emph{maximum likelihood} (ML) estimators for the mean and the variance of a univariate Gaussian distribution are
\begin{align}
\mu_{\text{ML}} &= \dfrac{1}{N} \sum\displaylimits_{n = 1}^{N} x_n \qquad \text{(sample mean)} \\
\sigma^2_{\text{ML}} &= \dfrac{1}{N} \sum\displaylimits_{n = 1}^{n}\bigl(x_n - \mu_{\text{ML}}\bigl)^2 \qquad \text{(sample variance)} \, .
\end{align}
Show that the expectation values of these two maximum likelihood estimators (with respect to the data set values $x_1, \dots, x_N$) are given by
\begin{align}
\mathbb{E}\bigl[\mu_{\text{ML}}\bigl] &= \mu \\
\mathbb{E}\bigl[\sigma^2_{\text{ML}}\bigl] &= \left(\frac{N - 1}{N}\right) \sigma^2
\end{align}
As an intermediate result, start by showing that
\begin{align}
\mathbb{E}\bigl[x_n x_m\bigl] = \mu^2 + \delta_{nm} \sigma^2 \, ,
\label{eq:interResult}
\end{align}
where $x_n$ and $x_m$ are two independent and identically distributed (i.i.d.) univariate random variables $x_n \sim \mathcal{N}\left(x_n \,\bigl| \, \mu, \sigma^2\right)$ , $x_m\sim \mathcal{N}(x_m \, \bigl| \, \mu, \sigma^2)$ and $\delta_{nm}$ is the Kronecker delta.
It is defined as
\begin{align}
\delta_{nm} = \begin{cases}
1 & \text{for} \, n = m \\
0 & \text{for} \, n \neq m \, .
\end{cases}
\end{align}
\end{mybox_tc3}



\end{document}
