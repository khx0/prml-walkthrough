\documentclass[11pt,DINA4, fleqn]{amsart}
\pagestyle{empty}
\newcommand{\R}{\Bbb{R}}
\newtheorem{thm}{Frage}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{bm} % bold math symbols

\usepackage{wasysym}
\usepackage[english]{babel}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{geometry}
\geometry{hmargin=2.5cm,vmargin={2cm,1cm}}

\usepackage{blindtext}
\usepackage{multirow,booktabs}

\usepackage{pifont}
\usepackage{dsfont}
\usepackage{enumitem}
%\setitemize{leftmargin=0.5cm} 
%\setlength{\itemindent}{1in}
\usepackage{fancybox,framed}

\usepackage{listings} \lstset{numbers=left, numberstyle=\tiny, numbersep=5pt} \lstset{language=c++} 

\usepackage{tcolorbox}

\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}

\usepackage{graphicx}

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.14,0.72,0.31}
\definecolor{MyBoxColor}{rgb}{0.9,0.9,0.9}
\newenvironment{shadedSmaller}{
  \def\FrameCommand{\fboxsep=\FrameSep \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-2\width\FrameRestore}}
{\endMakeFramed}

\newtcolorbox{mybox_tc2}[1]{colback=red!5!white,colframe=red!75!black,fonttitle=\bfseries,title=#1}

\newtcolorbox{mybox_tc3}[1]{colback=darkgreen!5!white,colframe=darkgreen!75!black,fonttitle=\bfseries,title=#1}


\newenvironment{shadedSmallerPadding}{
  \def\FrameCommand{\fboxsep=0.15cm \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-1.1\width\FrameRestore}}
{\endMakeFramed}

\usepackage{faktor}

\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

%----------------------------------------------------------------------------------------------------------------

\def\vw{\boldsymbol{w}\xspace}
\def\vb{\boldsymbol{b}\xspace}
\def\vy{\boldsymbol{y}\xspace}
\def\mA{\boldsymbol{A}\xspace}
\def\mV{\boldsymbol{V}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Ldot}{\overset{\textbf{.}\kern0.23em}{\mathbf{L}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%\begin{tikzpicture}[normal line/.style={->},font=\scriptsize] \path[normal line] (m-1-1) edge (m-1-2); %\end{tikzpicture}

\tikzset{node distance = 2cm, auto}


%                                      $\mathds{Z}/n\mathds{Z}$

\begin{flushleft}
{\sc \Large PRML Notes} \hfill \today \\
\medskip
Nikolas Schnellb√§cher \underline{\hspace{6.53in}} \\
\end{flushleft}

\section*{ad 1.2.5 Curve fitting revisited}
We consider a set of training data $\{\boldsymbol{\mathsf{x}}, \boldsymbol{\mathsf{t}}\}$, where
\begin{align}
\boldsymbol{\mathsf{x}} &= (x_1, \dots, x_N)^T \, , \\
\boldsymbol{\mathsf{t}} &= (t_1, \dots, t_N)^T
\end{align}
are the input variables $x_n$ and their corresponding target values $t_n$, respectively.
The central idea is using probability distribution to  express our uncertainty over the value of the target variable $t$, \textit{i.e.} we state
\begin{align}
p(t \, | \, x, \bm{w}, \beta) = \mathcal{N}\left(
t \, \bigl| \, y(x,\bm{w}), \beta^{-1}\right) \, .
\label{eq:tUncertainty}
\end{align}
The parameter $\beta$ is called \emph{precision} and correspond to the inverse variance of the distribution.

In curve fitting, we use the existing training data $\{\boldsymbol{\mathsf{x}}, \boldsymbol{\mathsf{t}}\}$ to infer the unknown parameters $\bm{w}$ and $\beta$.
We will below show how this works using a \emph{Maximum Likelihood} ansatz.
We assume that the data is drawn independently from the distribution \eqref{eq:tUncertainty}. Then the likelihood function is given by
\begin{align}
p(\boldsymbol{\mathsf{t}} \, | \, \boldsymbol{\mathsf{x}}, \bm{w}, \beta) = 
\prod\displaylimits_{n = 1}^{N}
\mathcal{N}\left(
t_n \, \bigl| \, y(x_n,\bm{w}), \beta^{-1}\right)
\label{eq:likelihoodFunc}
\end{align}
Instead of maximizing the likelihood function directly it is convenient to maximize its logarithm.
\begin{align}
\ln\biggl(p(\boldsymbol{\mathsf{t}} \, | \, \boldsymbol{\mathsf{x}}, \bm{w}, \beta)\biggl) &= \ln\left(\,
\prod\displaylimits_{n=1}^{N} \dfrac{1}{\sqrt{2\pi \beta^{-1}}} \, \exp\left[
-\dfrac{\beta}{2}\biggl(t_n - y(x_n,\bm{w})\biggl)^2
\right]
\right) \\
&=
\sum\displaylimits_{n=1}^{N}
\left[
\ln\left(\dfrac{1}{\sqrt{2\pi \beta^{-1}}}\right) -
\dfrac{\beta}{2}\biggl(y(x_n, \bm{w}) - t_n\biggl)^2 \,
\right] \\
&=
-\dfrac{\beta}{2}\sum\displaylimits_{n=1}^{N} \biggl(y(x_n, \bm{w}) - t_n\biggl)^2 +
N \ln\left(\left(\dfrac{\beta}{2\pi}\right)^{1/2}\right) \\
&= 
-\dfrac{\beta}{2}\sum\displaylimits_{n=1}^{N} \biggl(y(x_n, \bm{w}) - t_n\biggl)^2 +
\dfrac{N}{2}\ln(\beta)
- \dfrac{N}{2}\ln(2\pi)
\label{eq:logLikelihood}
\end{align}
We start by considering the inferred solution for the parameter weights $\bm{w}$ according to the maximum likelihood principle and will call this solution $\bm{w}_{\text{ML}}$. For this we need to maximize the log likelihood from equation \eqref{eq:logLikelihood} w.r.t to $\bm{w}$.
Since the last two terms do not depend on $\bm{w}$ we can omit them from this consideration. Second, scaling the log likelihood by a positive constant coefficient does not alter the solution of this maximization, \textit{i.e.} we can replace the $\beta/ 2$ in front of the summation by simply $1/2$. And finally, instead of maximizing the log likelihood, we can equivalently minimize the negative log likelihood. In summary the function that we now seek to minimize reads
\begin{align}
\dfrac{1}{2} \sum\displaylimits_{n=1}^{N} \biggl(y(x_n, \bm{w}) - t_n\biggl)^2 =: E(\bm{w}) \, ,
\end{align}
which reduces to the well known sum-of-squares error function $E(\bm{w})$.
This means that maximizing the likelihood is equivalent, so far as we are concerned with determining $\bm{w}$, to minimizing the \emph{sum-of-squares} error function.
Or in other words: The \emph{sum-of-squares} error function arises naturally in the context of maximum likelihood under the assumption of a Gaussian noise model and statistically independent data samples. We find $\bm{w}$ as before by solving the corresponding linear system.

We can then equally use the maximum likelihood ansatz to determine the precision parameter $\beta_{\text{ML}}$. The derivative of equation \eqref{eq:logLikelihood} w.r.t to $\beta$ yields
\begin{align}
\dfrac{\partial}{\partial \beta} \ln\biggl(p(\boldsymbol{\mathsf{t}} \, | \, \boldsymbol{\mathsf{x}}, \bm{w}, \beta)\biggl) &= -\dfrac{1}{2}\sum\displaylimits_{n=1}^{N} \biggl(y(x_n, \bm{w}) - t_n\biggl)^2
+\,\, \dfrac{N}{2\beta} \stackrel{!}{=} 0 \, .
\end{align}
Solving the last expression for $\beta$, we find
\begin{align}
\beta_{\text{ML}} = \dfrac{N}{\sum\displaylimits_{n=1}^{N} \bigl(y(x_n, \bm{w}_{\text{ML}}) - t_n\bigl)^2} \, ,
\end{align}
or equivalently
\begin{align}
\dfrac{1}{\beta_{\text{ML}}} = \dfrac{1}{N} \sum\displaylimits_{n=1}^{N} \biggl(y(x_n, \bm{w}_{\text{ML}}) - t_n\biggl)^2 \, .
\end{align}
In this maximum likelihood ansatz, we first determine $\bm{w}_{\text{ML}}$ and use it subsequently for the determination of $\beta_{\text{ML}}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{figure_1_16.pdf}
	\caption{Schematic illustration of the uncertainty of the target variable $t$ as specified by a Gaussian conditional distribution as specified in equation \eqref{eq:tUncertainty}, for which the mean is given by the polynomial function $y(x,\bm{w})$ and the precision is set by the parameter $\beta$.
		\label{fig:figure_1_16}}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{mybox_tc3}{Probabilistic curve fitting - maximum likelihood approach}
We consider dataset $\{\boldsymbol{\mathsf{x}}, \boldsymbol{\mathsf{t}}\}$
and a (polynomial) model function $y(x,\bm{w})$.
Assuming that
\begin{itemize}
	\item the data points $\{(x_n, t_n)\}$ are statistically independent
	\item and distributed according to
	\begin{align}
	t_n  \sim p(t_n \, | \, x_n, \bm{w}, \beta) = \mathcal{N}\left(
	t_n \, \bigl| \, y(x_n,\bm{w}), \beta^{-1}\right) 
	\end{align}
	a Gaussian distribution with mean $\mu = y(x_n, \bm{w})$ and variance $\sigma^2 = \beta^{-1}$, maximizing the likelihood (log likelihood), is equivalent to minimzing the \emph{sum-of-squares} error function
	\begin{align}
	E(\bm{w}) =\dfrac{1}{2} \sum\displaylimits_{n=1}^{N} \biggl(y(x_n, \bm{w}) - t_n\biggl)^2.
	\end{align}
\end{itemize}
\end{mybox_tc3}



\end{document}
