\documentclass[11pt,DINA4, fleqn]{amsart}
\pagestyle{empty}
\newcommand{\R}{\Bbb{R}}
\newtheorem{thm}{Frage}
\usepackage{amsmath,amssymb,amsthm,amsfonts}

\usepackage{wasysym}
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{geometry}
\geometry{hmargin=2.5cm,vmargin={2cm,1cm}}

\usepackage{blindtext}
\usepackage{multirow,booktabs}

\usepackage{pifont}
\usepackage{dsfont}
\usepackage{enumitem}
%\setitemize{leftmargin=0.5cm} 
%\setlength{\itemindent}{1in}
\usepackage{fancybox,framed}

\usepackage{listings} \lstset{numbers=left, numberstyle=\tiny, numbersep=5pt} \lstset{language=c++} 

\usepackage{tcolorbox}

\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.14,0.72,0.31}
\definecolor{MyBoxColor}{rgb}{0.9,0.9,0.9}
\newenvironment{shadedSmaller}{
  \def\FrameCommand{\fboxsep=\FrameSep \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-2\width\FrameRestore}}
{\endMakeFramed}

\newtcolorbox{mybox_tc2}[1]{colback=red!5!white,colframe=red!75!black,fonttitle=\bfseries,title=#1}

\newtcolorbox{mybox_tc3}[1]{colback=darkgreen!5!white,colframe=darkgreen!75!black,fonttitle=\bfseries,title=#1}


\newenvironment{shadedSmallerPadding}{
  \def\FrameCommand{\fboxsep=0.15cm \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-1.1\width\FrameRestore}}
{\endMakeFramed}

\usepackage{faktor}

\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

%----------------------------------------------------------------------------------------------------------------

\def\vw{\boldsymbol{w}\xspace}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Ldot}{\overset{\textbf{.}\kern0.23em}{\mathbf{L}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}



%\begin{tikzpicture}[normal line/.style={->},font=\scriptsize] \path[normal line] (m-1-1) edge (m-1-2); %\end{tikzpicture}

\tikzset{node distance=2cm, auto}




%                                      $\mathds{Z}/n\mathds{Z}$

\begin{flushleft}
{\sc \Large PRML Exercise 1.1 \& 1.2} \hfill \today \\
\medskip
Nikolas Schnellb√§cher \underline{\hspace{6.53in}} \\
\end{flushleft}

\section*{Exercise 1.1 -- Polynomial Least Squares Curve Fitting}
This exercise considers polynomial least squares curve fitting. 
For this purpose we consider the sum of squares error function as defined in equation 1.2 of chapter 1, which we repeat below
\begin{align}
E(\vw) = \dfrac{1}{2} \sum \displaylimits_{n = 1}^{N}
\biggl(y(x_n, \vw) - t_n)\biggl)^2 \, .
\label{eq:error1}
\end{align}
Here we consider the model function $y$ to be a polynomial of degree $M$ in $x$ with coefficients $\vw = (w_0, w_1, w_2, \dots , w_M)^{T}$, such that
\begin{align}
y(x,\vw) = w_0 + w_1 x + w_2 x^2 + \dotsc + w_M x^M = \sum \displaylimits_{j = 0}^{M} w_j x^j
\label{eq:polynom}
\end{align}
is the polynomial model function (also known as fit function) with $M + 1$ summands, containing $M+1$ parameters $w_j$. These parameters are also often referred to as weights.

\begin{mybox_tc3}{Exercise 1.1 -- Polynomial Least Squares Curve Fitting}
Consider the sum-of-squares error function given in equation \eqref{eq:error1}
with a polynomial model function $y(x,\vw)$ of order $M$ as specified in equation \eqref{eq:polynom}.
Given a set of $N$ datapoints $(x_n, t_n)$ with $n = 1, \dotsc, N$,
 derive the linear system, which minimizes this
error function, \textit{i.e.} show how to find the optimal solution for $\vw$
which minimizes the sum-of-squares error
\begin{align}
\vw^{*} =  \underset{\vw}{\text{argmin}}\biggl[ E(\vw)\biggl] \, ,
\end{align}
given the $N$ data points $(x_n, t_n)$.
\end{mybox_tc3}

\subsection*{Solution}
To find the optimal solution $\vw^*$ which minimizes the energy functional, we take the partial derivatives of $E(\vw)$ with respect to the weight parameters $w_j$ for all $j$ and set them to zero, to find the extremal solution. This leads to a set of $M+1$ equations, which happen to be linear in the coefficients $w_j$. Thus we can solve polynomial least squares fitting by solving a simple linear system.

\begin{align}
\dfrac{\partial E}{\partial w_0} &= \sum \displaylimits_{n = 1}^{N}
\biggl(\underbrace{w_0 + w_1 x_n + w_2 x_n^2 + \dotsc + w_M x_n^M}_{=y(x_n,\vw)} - t_n\biggl) \cdot 1
\overset{!}{=} 0 \\
\dfrac{\partial E}{\partial w_1} &= \sum \displaylimits_{n = 1}^{N}
\biggl(w_0 + w_1 x_n + w_2 x_n^2 + \dotsc + w_M x_n^M - t_n\biggl) \cdot x_n
\overset{!}{=} 0 \\
\phantom{\dfrac{\partial E}{\partial w_M}}&\,\,\,\vdots \\
\dfrac{\partial E}{\partial w_M} &= \sum \displaylimits_{n = 1}^{N}
\biggl(w_0 + w_1 x_n + w_2 x_n^2 + \dotsc + w_M x_n^M - t_n\biggl) \cdot x_n^M
\overset{!}{=} 0
\end{align}
Rearranging all $M+1$ equations by dragging the $n$ summation over all terms, we can identically rewrite this set of equations as
\begin{align}
w_0 N + w_1 \sum\displaylimits_{n = 1}^{N}x_n
+ w_2 \sum\displaylimits_{n = 1}^{N}x_n^2 + \dotsc + w_M \sum\displaylimits_{n = 1}^{N}x_n^M  &= \sum\displaylimits_{n = 1}^{N} t_n \\
w_0 \sum\displaylimits_{n = 1}^{N} x_n + w_1 \sum\displaylimits_{n = 1}^{N}x_n^2
+ w_2 \sum\displaylimits_{n = 1}^{N}x_n^3 + \dotsc + w_M \sum\displaylimits_{n = 1}^{N}x_n^{M + 1}  &= \sum\displaylimits_{n = 1}^{N} x_n t_n
 \\
\phantom{\dfrac{\partial E}{\partial w_M}}&\,\,\,\vdots \\
w_0 \sum\displaylimits_{n = 1}^{N} x_n + w_1 \sum\displaylimits_{n = 1}^{N}x_n^2
+ w_2 \sum\displaylimits_{n = 1}^{N}x_n^3 + \dotsc + w_M \sum\displaylimits_{n = 1}^{N}x_n^{M + 1}  &= \sum\displaylimits_{n = 1}^{N} x_n t_n
\end{align}


\end{document}









