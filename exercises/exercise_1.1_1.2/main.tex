\documentclass[11pt,DINA4, fleqn]{amsart}
\pagestyle{empty}
\newcommand{\R}{\Bbb{R}}
\newtheorem{thm}{Frage}
\usepackage{amsmath,amssymb,amsthm,amsfonts}

\usepackage{wasysym}
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{geometry}
\geometry{hmargin=2.5cm,vmargin={2cm,1cm}}

\usepackage{blindtext}
\usepackage{multirow,booktabs}

\usepackage{pifont}
\usepackage{dsfont}
\usepackage{enumitem}
%\setitemize{leftmargin=0.5cm} 
%\setlength{\itemindent}{1in}
\usepackage{fancybox,framed}

\usepackage{listings} \lstset{numbers=left, numberstyle=\tiny, numbersep=5pt} \lstset{language=c++} 

\usepackage{tcolorbox}

\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.14,0.72,0.31}
\definecolor{MyBoxColor}{rgb}{0.9,0.9,0.9}
\newenvironment{shadedSmaller}{
  \def\FrameCommand{\fboxsep=\FrameSep \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-2\width\FrameRestore}}
{\endMakeFramed}

\newtcolorbox{mybox_tc2}[1]{colback=red!5!white,colframe=red!75!black,fonttitle=\bfseries,title=#1}

\newtcolorbox{mybox_tc3}[1]{colback=darkgreen!5!white,colframe=darkgreen!75!black,fonttitle=\bfseries,title=#1}


\newenvironment{shadedSmallerPadding}{
  \def\FrameCommand{\fboxsep=0.15cm \colorbox{MyBoxColor}}
  \MakeFramed {\advance\hsize-1.1\width\FrameRestore}}
{\endMakeFramed}

\usepackage{faktor}

\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

%----------------------------------------------------------------------------------------------------------------

\def\vw{\boldsymbol{w}\xspace}
\def\vb{\boldsymbol{b}\xspace}
\def\mA{\boldsymbol{A}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Ldot}{\overset{\textbf{.}\kern0.23em}{\mathbf{L}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}



%\begin{tikzpicture}[normal line/.style={->},font=\scriptsize] \path[normal line] (m-1-1) edge (m-1-2); %\end{tikzpicture}

\tikzset{node distance=2cm, auto}




%                                      $\mathds{Z}/n\mathds{Z}$

\begin{flushleft}
{\sc \Large PRML Exercise 1.1 \& 1.2} \hfill \today \\
\medskip
Nikolas Schnellb√§cher \underline{\hspace{6.53in}} \\
\end{flushleft}

\section*{Exercise 1.1 -- Polynomial Least Squares Curve Fitting}
This exercise considers polynomial least squares curve fitting. 
For this purpose we consider the sum of squares error function as defined in equation 1.2 of chapter 1, which we repeat below
\begin{align}
E(\vw) = \dfrac{1}{2} \sum \displaylimits_{n = 1}^{N}
\biggl(y(x_n, \vw) - t_n)\biggl)^2 \, .
\label{eq:error1}
\end{align}
Here we consider the model function $y$ to be a polynomial of degree $M$ in $x$ with coefficients $\vw = (w_0, w_1, w_2, \dots , w_M)^{T}$, such that
\begin{align}
y(x,\vw) = w_0 + w_1 x + w_2 x^2 + \dotsc + w_M x^M = \sum \displaylimits_{j = 0}^{M} w_j x^j
\label{eq:polynom}
\end{align}
is the polynomial model function (also known as fit function) with $M + 1$ summands, containing $M+1$ parameters $w_j$. These parameters are also often referred to as weights.

\begin{mybox_tc3}{Exercise 1.1 -- Polynomial Least Squares Curve Fitting}
Consider the sum-of-squares error function given in equation \eqref{eq:error1}
with a polynomial model function $y(x,\vw)$ of order $M$ as specified in equation \eqref{eq:polynom}.
Given a set of $N$ data points $(x_n, t_n)$ with $n = 1, \dotsc, N$,
 derive the linear system, which minimizes this
error function, \textit{i.e.} show how to find the optimal solution $\vw^*$,
which minimizes the sum-of-squares error
\begin{align}
\vw^{*} =  \underset{\vw}{\text{argmin}}\biggl[ E(\vw)\biggl] \, ,
\end{align}
given the $N$ data points $(x_n, t_n)$.
\end{mybox_tc3}

\subsection*{Solution}
To find the optimal solution $\vw^*$ which minimizes the energy functional, we take the partial derivatives of $E(\vw)$ with respect to the weight parameters $w_j$ for all $j$ and set them to zero, to find the extremal solution. This leads to a set of $M+1$ equations, which happen to be linear in the coefficients $w_j$. Thus we can solve polynomial least squares fitting by solving a simple linear system.

\begin{align}
\dfrac{\partial E}{\partial w_0} &= \sum \displaylimits_{n = 1}^{N}
\biggl(\underbrace{w_0 + w_1 x_n + w_2 x_n^2 + \dotsc + w_M x_n^M}_{=y(x_n,\vw)} - t_n\biggl) \cdot 1
\overset{!}{=} 0 \\
\dfrac{\partial E}{\partial w_1} &= \sum \displaylimits_{n = 1}^{N}
\biggl(w_0 + w_1 x_n + w_2 x_n^2 + \dotsc + w_M x_n^M - t_n\biggl) \cdot x_n
\overset{!}{=} 0 \\
\phantom{\dfrac{\partial E}{\partial w_M}}&\,\,\,\vdots \\
\dfrac{\partial E}{\partial w_M} &= \sum \displaylimits_{n = 1}^{N}
\biggl(w_0 + w_1 x_n + w_2 x_n^2 + \dotsc + w_M x_n^M - t_n\biggl) \cdot x_n^M
\overset{!}{=} 0
\end{align}
Rearranging all $M+1$ equations by dragging the $n$ summation over all terms, we can identically rewrite this set of equations as
\begin{align}
w_0 N + w_1 \sum\displaylimits_{n = 1}^{N}x_n
+ w_2 \sum\displaylimits_{n = 1}^{N}x_n^2 + \dotsc + w_M \sum\displaylimits_{n = 1}^{N}x_n^M  &= \sum\displaylimits_{n = 1}^{N} t_n \\
w_0 \sum\displaylimits_{n = 1}^{N} x_n + w_1 \sum\displaylimits_{n = 1}^{N}x_n^2
+ w_2 \sum\displaylimits_{n = 1}^{N}x_n^3 + \dotsc + w_M \sum\displaylimits_{n = 1}^{N}x_n^{M + 1}  &= \sum\displaylimits_{n = 1}^{N} x_n t_n
 \\
\phantom{\dfrac{\partial E}{\partial w_M}}&\,\,\,\vdots \\
w_0 \sum\displaylimits_{n = 1}^{N} x_n^M + w_1 \sum\displaylimits_{n = 1}^{N}x_n^{M+1}
+ w_2 \sum\displaylimits_{n = 1}^{N}x_n^{M+2} + \dotsc + w_M \sum\displaylimits_{n = 1}^{N}x_n^{2M}  &= \sum\displaylimits_{n = 1}^{N} x_n^M t_n
\end{align}
This is a set of $M+1$ linear equations in $w_0, w_1, w_2, \dotsc, w_M$. We can immediately rewrite this in matrix form and find.
\begin{align}
\underbrace{
\begin{pmatrix}
N & \sum\displaylimits_{n = 1}^{N}x_n & \sum\displaylimits_{n = 1}^{N}x_n^2 & \dots & \sum\displaylimits_{n = 1}^{N}x_n^M \\
\sum\displaylimits_{n = 1}^{N}x_n & \sum\displaylimits_{n = 1}^{N}x_n^2 &
\sum\displaylimits_{n = 1}^{N}x_n^3 & \dots &
\sum\displaylimits_{n = 1}^{N}x_n^{M + 1} \\
\vdots & \vdots & \vdots && \vdots \\ 
\sum\displaylimits_{n = 1}^{N}x_n^M &
\sum\displaylimits_{n = 1}^{N}x_n^{M+1} &
\sum\displaylimits_{n = 1}^{N}x_n^{M+2} &
\dotsc &  \sum\displaylimits_{n = 1}^{N}x_n^{2M}
\end{pmatrix}}_{=\mA} \cdot
\underbrace{\begin{pmatrix}
w_0 \\ w_1 \\ \vdots \\ w_M
\end{pmatrix}}_{=\vw}
=
\underbrace{\begin{pmatrix}
\sum\displaylimits_{n = 1}^{N} t_n \\
\sum\displaylimits_{n = 1}^{N} x_n t_n \\
\vdots \\
\sum\displaylimits_{n = 1}^{N} x_n^M t_n 
\end{pmatrix}}_{=\vb} \, \in \mathbb{R}^{M+1}
\end{align}
By solving the linear system $\mA \cdot \vw = \vb$ for the weights $\vw$ thus gives us the optimal solution, which minimizes the sum of squared errors.

\begin{mybox_tc3}{Polynomial Least Squares Curve Fitting}
	For $N$ given data points $(x_n, t_n)$ and a polynomial fitting model of order $M$,
	the optimal solution $\vw^*$ to the polynomial least squares curve fitting problem is the solution of the linear system
	\begin{align}
	\mA \cdot \vw = \vb \quad \Longleftrightarrow \quad \sum \displaylimits_{j = 0}^{M} A_{ij} \cdot w_j = b_i  \qquad \forall\quad i \in \{0, 1, 2, \dotsc, M\}
	\end{align}
	where
	\begin{align}
	A_{ij} &= \sum \displaylimits_{n = 1}^{N} (x_n)^{i + j} \\
	(\vw)_j &= w_j \\
	b_i &= \sum \displaylimits_{n = 1}^{N} (x_n)^{i} \, t_n
	\end{align}
	Both indices $i$ and $j$ run from $0, 1, 2, \dotsc, M$.
	The matrix $\mA$ is obviously symmetric by construction $A_{ij} = A_{ji}$.
\end{mybox_tc3}


\end{document}









